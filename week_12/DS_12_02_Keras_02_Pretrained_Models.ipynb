{"cells":[{"cell_type":"markdown","id":"13b30d7e-8769-497a-8ff3-410157a58567","metadata":{},"source":["# 02. Pretrained Models"]},{"cell_type":"markdown","id":"373dcde3-aeb9-47f2-95b4-832192acc8de","metadata":{},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","id":"1da2417d-cf35-4426-933e-0f94ab2704b1","metadata":{},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","id":"df933b81-9331-4f83-a691-f4000da40400","metadata":{},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":1,"id":"ae3d4906-1ca0-4da4-8e9d-0318c7cf80f3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","id":"feb9d8f2-3a9b-41fe-a4bd-2eb9c0a762f3","metadata":{},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":2,"id":"4ac3c0af-d7d2-4b86-b360-f43231471500","metadata":{},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","id":"feef4bee-e4f2-4ecd-9677-1208dce1700b","metadata":{},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":3,"id":"0c1537cb-03a6-4f04-b906-ab9ed76d26ee","metadata":{},"outputs":[],"source":["from keras.applications import ResNet50\n","from keras.applications.resnet50 import preprocess_input"]},{"cell_type":"markdown","id":"a4533766-4ae0-432e-b6cd-e32b558fadf0","metadata":{},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","id":"acfc926b-9b40-4ead-81aa-b6f4e3dbfd88","metadata":{},"source":["Here, we will define constants that we will be using throughout the rest of the lab. \n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2. \n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":4,"id":"eadf578c-16b2-425a-885e-1f5202becc12","metadata":{},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","id":"7a4a47fa-9090-4e6e-8c78-d98b03cfc38d","metadata":{},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","id":"352ef6bc-0d38-4949-808a-a37b0b051871","metadata":{},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained Deep Neural Networks"]},{"cell_type":"markdown","metadata":{},"source":["https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html  \n","https://www.mathworks.com/help/deeplearning/ref/resnet50.html"]},{"cell_type":"markdown","metadata":{},"source":["<img src='img/pretrained_20b.png' width=600px><br>\n","<img src='img/resnet50.png' width=600px>"]},{"cell_type":"markdown","metadata":{},"source":["You can take a pretrained image classification neural network that has already learned to extract powerful and informative features from natural images and use it as a starting point to learn a new task. The majority of the pretrained neural networks are trained on a subset of the ImageNet database, which is used in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). These neural networks have been trained on more than a million images and can classify images into 1000 object categories, such as keyboard, coffee mug, pencil, and many animals. Using a pretrained neural network with transfer learning is typically much faster and easier than training a neural network from scratch."]},{"cell_type":"markdown","metadata":{},"source":["<hr>"]},{"cell_type":"code","execution_count":5,"id":"72a935a2-9f94-42e6-b764-e01de7ac1af9","metadata":{},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","id":"0e7df7c6-d10f-466d-bc7b-738eca43d839","metadata":{},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":6,"id":"14dd3fdb-bc4e-4af5-8c3b-24292f73dcdf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30000 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'data/concrete_crack/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"60d98b33-d357-4a11-ad61-63055ba10334","metadata":{},"source":["**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation."]},{"cell_type":"markdown","id":"e4537a5e-adcf-4722-95c0-9b277b445bd6","metadata":{},"source":["Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":7,"id":"3d4b940f-2dc4-44f0-b26e-a04906e0bcfb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 9500 images belonging to 2 classes.\n"]}],"source":["validation_generator = data_generator.flow_from_directory(\n","    'data/concrete_crack/valid/',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"c20de955-f27d-461c-8c71-9b0189014f71","metadata":{},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","id":"ade32f00-7084-47c7-8b46-6350be8350bb","metadata":{},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":8,"id":"e6e60cc3-76f1-4096-ab2f-2ea30f49700c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"]}],"source":["model = Sequential()"]},{"cell_type":"markdown","id":"0fdc30dd-813a-4b2c-b23d-e48b0f3b5530","metadata":{},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":9,"id":"391d3165-5d15-407d-bb54-9262b92ddffd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","id":"382b4886-4d8b-4451-baca-462437fb18e6","metadata":{},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":10,"id":"3d6f3e8a-b880-44cb-96f6-aaa53a23b987","metadata":{},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","id":"79deafb4-323b-4cc6-80e8-97a9d8d3703e","metadata":{},"source":["You can access the model's layers using the *layers* attribute of our model object. \n"]},{"cell_type":"code","execution_count":11,"id":"f7a33afa-9d85-47e6-8746-5bf8a6378fae","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.src.engine.functional.Functional at 0x252c2789fd0>,\n"," <keras.src.layers.core.dense.Dense at 0x252cb950e10>]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","id":"934150ad-a32e-4d67-99cb-e3c8c6f63292","metadata":{},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","id":"483de117-cda0-4b1e-bf37-18871f0c8977","metadata":{},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":12,"id":"93ad07b7-a873-421d-a90f-9f6a8f1644d8","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.src.engine.input_layer.InputLayer at 0x252e84b3bd0>,\n"," <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x252c0b91a50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252bef83290>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c141f2d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c0a73650>,\n"," <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x252ffa1a350>,\n"," <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x252c0b93390>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c145b750>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c141d490>,\n"," <keras.src.layers.core.activation.Activation at 0x252c14fde10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c14a5690>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1441d50>,\n"," <keras.src.layers.core.activation.Activation at 0x252bf36cd50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1442dd0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c14fd550>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252e850d9d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c150b790>,\n"," <keras.src.layers.merging.add.Add at 0x252c1443910>,\n"," <keras.src.layers.core.activation.Activation at 0x252c141ff10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1529a90>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c13f38d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252e85094d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c152aa10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0eb1e50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c14415d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c15238d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1483150>,\n"," <keras.src.layers.merging.add.Add at 0x252bf0ecad0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c0eb00d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c145be90>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c14fe8d0>,\n"," <keras.src.layers.core.activation.Activation at 0x25295c318d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c155fb10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0eb1dd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1483990>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1441910>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0eb0090>,\n"," <keras.src.layers.merging.add.Add at 0x252c14fe910>,\n"," <keras.src.layers.core.activation.Activation at 0x252bec2bed0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1577910>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252bec6c4d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1577c90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c158e390>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1574f50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1580050>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1577490>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c278a610>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0eb31d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c13f1f90>,\n"," <keras.src.layers.merging.add.Add at 0x252c278be90>,\n"," <keras.src.layers.core.activation.Activation at 0x252c278a390>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c158df10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c158da50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c15756d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1569810>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1569f50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c278b950>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c1541150>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1551a90>,\n"," <keras.src.layers.merging.add.Add at 0x252c1443650>,\n"," <keras.src.layers.core.activation.Activation at 0x252c14414d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2779090>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1551210>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1581490>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c279be90>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c145b410>,\n"," <keras.src.layers.core.activation.Activation at 0x252c0b68f10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27ad2d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2778210>,\n"," <keras.src.layers.merging.add.Add at 0x252c27b3850>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27b2410>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27b7fd0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2799290>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1483850>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27c7950>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0eb3b90>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1481310>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27cde10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2778350>,\n"," <keras.src.layers.merging.add.Add at 0x252c27cfbd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1541d50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27dcf10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27de8d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1482650>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27fa2d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27cf9d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27fae90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27dcfd0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27ea1d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1551110>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27e8490>,\n"," <keras.src.layers.merging.add.Add at 0x252c27d5390>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27d7810>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27c5790>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27cabd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27c4d90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27fa3d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c14e3450>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27eb210>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27acfd0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c155eb50>,\n"," <keras.src.layers.merging.add.Add at 0x252c13f26d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27b1c90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2819ad0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0b91b90>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27ad390>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2825c90>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c1571c50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2827850>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2833510>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27f9f90>,\n"," <keras.src.layers.merging.add.Add at 0x252c2789cd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2825b10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c283e810>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2832dd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c283fe10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c284b150>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27dedd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c1532190>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2848890>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c284bb50>,\n"," <keras.src.layers.merging.add.Add at 0x252c2819bd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c285aed0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2863690>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c0a72a10>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2830550>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c286bad0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27ca6d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2869d10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c2860d50>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c28614d0>,\n"," <keras.src.layers.merging.add.Add at 0x252c2858d50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c284ae50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c282ee50>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c28305d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2861850>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252becbf0d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c282da10>,\n"," <keras.src.layers.core.activation.Activation at 0x252c281ac10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c27ad5d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c27fbbd0>,\n"," <keras.src.layers.merging.add.Add at 0x252c28254d0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c65d31d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c65d2190>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c286a750>,\n"," <keras.src.layers.core.activation.Activation at 0x252c65f31d0>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c65f7f10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c65f7d50>,\n"," <keras.src.layers.core.activation.Activation at 0x252c27d5f10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c65e3250>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c65fd490>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c65e3fd0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2869b90>,\n"," <keras.src.layers.merging.add.Add at 0x252c285a150>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2851610>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c282e110>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c661be10>,\n"," <keras.src.layers.core.activation.Activation at 0x252c13f1e50>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c661c690>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c2868110>,\n"," <keras.src.layers.core.activation.Activation at 0x252c65f6590>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c662e350>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c15415d0>,\n"," <keras.src.layers.merging.add.Add at 0x252c65f6cd0>,\n"," <keras.src.layers.core.activation.Activation at 0x252c2860710>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c65f6d10>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c141da10>,\n"," <keras.src.layers.core.activation.Activation at 0x252c6642c90>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c66325d0>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c6643790>,\n"," <keras.src.layers.core.activation.Activation at 0x252c662ca10>,\n"," <keras.src.layers.convolutional.conv2d.Conv2D at 0x252c6628090>,\n"," <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x252c6618b50>,\n"," <keras.src.layers.merging.add.Add at 0x252c660ad90>,\n"," <keras.src.layers.core.activation.Activation at 0x252c65f7010>,\n"," <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x252c65f1510>]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","id":"c9b50534-db82-48ff-a966-db9c9db20b83","metadata":{},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":13,"id":"e9ab99d5-d322-41e7-9c67-7da3d02c0b81","metadata":{},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","id":"936f1a04-0bcc-4a29-935e-e20d23cd76cd","metadata":{},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":14,"id":"80e16337-42ed-4ee6-ab2c-3fa90719d4ff","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23591810 (90.00 MB)\n","Trainable params: 4098 (16.01 KB)\n","Non-trainable params: 23587712 (89.98 MB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"31c39eb0-b6c6-4f14-9c26-b861e3d071a0","metadata":{},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":15,"id":"4b9d8738-412b-4b18-afec-ff07dd316960","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"]}],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"74173ec3-81e4-41cb-84f0-047d562b195c","metadata":{},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":16,"id":"d0f49844-0ce6-468a-adc0-17c2d9ddbab5","metadata":{},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","id":"7f09403d-0dd1-492b-8630-86fb412f6f5a","metadata":{},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#%pip instal scipy\n","import scipy"]},{"cell_type":"markdown","metadata":{},"source":["**Warning**: next code will be execute more than 20 minutes:"]},{"cell_type":"code","execution_count":18,"id":"0b0dc574-8803-44d7-b3d7-57026a9f74c0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\usman\\AppData\\Local\\Temp\\ipykernel_9920\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  fit_history = model.fit_generator(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","300/300 [==============================] - 572s 2s/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0061 - val_accuracy: 0.9983\n","Epoch 2/2\n","300/300 [==============================] - 569s 2s/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.0047 - val_accuracy: 0.9986\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","id":"8ddfe7fa-64dc-446e-90f1-4caf95352890","metadata":{},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","id":"acff630c-cf29-45b0-be4a-9fbcfb475b42","metadata":{},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":19,"id":"0424beb7-0c3f-4bb3-af40-14e852aaa92f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\usman\\Desktop\\122XSUP\\week_02\\.my_env_02\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","id":"d02a8457-6201-4efd-9f84-ac65812ba037","metadata":{},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]}],"metadata":{"kernelspec":{"display_name":".my_env_04","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
