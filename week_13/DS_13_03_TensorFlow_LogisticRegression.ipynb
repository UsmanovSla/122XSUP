{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\usman\\Desktop\\122XSUP\\week_04\\.my_env_04\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST data is a collection of hand-written digits that contains 60,000 examples for training and 10,000 examples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# Flatten images to 1-D vector of 784 features (28*28).\n",
    "num_features = 28*28\n",
    "\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset parameters\n",
    "num_classes = 10 # 0 to 9 digits\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 256\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "\n",
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function calculates the error between actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression (Wx + b).\n",
    "def logistic_regression(x):\n",
    "    # Apply softmax to normalize the logits to a probability distribution.\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Encode label to a one hot vector.\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "    # Clip prediction values to avoid log(0) error.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "\n",
    "    # Compute cross-entropy.\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers and Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "# Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "# Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "\n",
    "        pred = logistic_regression(x)\n",
    "\n",
    "        loss = cross_entropy(pred, y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "\n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss: 519.167969, accuracy: 0.863281\n",
      "step: 200, loss: 738.987244, accuracy: 0.804688\n",
      "step: 300, loss: 530.781494, accuracy: 0.835938\n",
      "step: 400, loss: 517.976379, accuracy: 0.855469\n",
      "step: 500, loss: 39.550777, accuracy: 0.960938\n",
      "step: 600, loss: 74.677307, accuracy: 0.917969\n",
      "step: 700, loss: 59.396957, accuracy: 0.925781\n",
      "step: 800, loss: 56.650833, accuracy: 0.929688\n",
      "step: 900, loss: 47.108589, accuracy: 0.937500\n",
      "step: 1000, loss: 59.829865, accuracy: 0.898438\n",
      "step: 1100, loss: 73.249832, accuracy: 0.917969\n",
      "step: 1200, loss: 54.066582, accuracy: 0.925781\n",
      "step: 1300, loss: 378.722137, accuracy: 0.730469\n",
      "step: 1400, loss: 33.758072, accuracy: 0.960938\n",
      "step: 1500, loss: 99.390778, accuracy: 0.910156\n",
      "step: 1600, loss: 47.263832, accuracy: 0.957031\n",
      "step: 1700, loss: 62.820263, accuracy: 0.925781\n",
      "step: 1800, loss: 76.750786, accuracy: 0.910156\n",
      "step: 1900, loss: 99.906105, accuracy: 0.906250\n",
      "step: 2000, loss: 131.545486, accuracy: 0.867188\n",
      "step: 2100, loss: 69.679062, accuracy: 0.957031\n",
      "step: 2200, loss: 149.866867, accuracy: 0.839844\n",
      "step: 2300, loss: 63.193508, accuracy: 0.957031\n",
      "step: 2400, loss: 56.321507, accuracy: 0.945312\n",
      "step: 2500, loss: 141.967651, accuracy: 0.859375\n",
      "step: 2600, loss: 44.506176, accuracy: 0.945312\n",
      "step: 2700, loss: 98.537491, accuracy: 0.878906\n",
      "step: 2800, loss: 92.184769, accuracy: 0.898438\n",
      "step: 2900, loss: 70.893158, accuracy: 0.925781\n",
      "step: 3000, loss: 44.597225, accuracy: 0.937500\n",
      "step: 3100, loss: 109.266769, accuracy: 0.867188\n",
      "step: 3200, loss: 64.444809, accuracy: 0.917969\n",
      "step: 3300, loss: 64.019531, accuracy: 0.933594\n",
      "step: 3400, loss: 21.011196, accuracy: 0.980469\n",
      "step: 3500, loss: 43.763210, accuracy: 0.945312\n",
      "step: 3600, loss: 49.691734, accuracy: 0.937500\n",
      "step: 3700, loss: 54.411938, accuracy: 0.957031\n",
      "step: 3800, loss: 140.466553, accuracy: 0.839844\n",
      "step: 3900, loss: 66.219337, accuracy: 0.929688\n",
      "step: 4000, loss: 60.465553, accuracy: 0.945312\n",
      "step: 4100, loss: 28.243481, accuracy: 0.964844\n",
      "step: 4200, loss: 46.447128, accuracy: 0.945312\n",
      "step: 4300, loss: 72.573410, accuracy: 0.941406\n",
      "step: 4400, loss: 57.921661, accuracy: 0.941406\n",
      "step: 4500, loss: 213.589233, accuracy: 0.824219\n",
      "step: 4600, loss: 119.883591, accuracy: 0.914062\n",
      "step: 4700, loss: 33.718895, accuracy: 0.957031\n",
      "step: 4800, loss: 45.291691, accuracy: 0.953125\n",
      "step: 4900, loss: 66.976685, accuracy: 0.929688\n",
      "step: 5000, loss: 53.967720, accuracy: 0.941406\n",
      "step: 5100, loss: 70.702225, accuracy: 0.933594\n",
      "step: 5200, loss: 47.217731, accuracy: 0.949219\n",
      "step: 5300, loss: 79.993134, accuracy: 0.921875\n",
      "step: 5400, loss: 73.312088, accuracy: 0.906250\n",
      "step: 5500, loss: 51.419785, accuracy: 0.929688\n",
      "step: 5600, loss: 145.346466, accuracy: 0.875000\n",
      "step: 5700, loss: 28.241488, accuracy: 0.968750\n",
      "step: 5800, loss: 57.995182, accuracy: 0.906250\n",
      "step: 5900, loss: 71.153183, accuracy: 0.906250\n",
      "step: 6000, loss: 69.062286, accuracy: 0.921875\n",
      "step: 6100, loss: 59.619186, accuracy: 0.914062\n",
      "step: 6200, loss: 34.507637, accuracy: 0.949219\n",
      "step: 6300, loss: 77.766556, accuracy: 0.945312\n",
      "step: 6400, loss: 64.101334, accuracy: 0.925781\n",
      "step: 6500, loss: 69.730270, accuracy: 0.941406\n",
      "step: 6600, loss: 82.327942, accuracy: 0.937500\n",
      "step: 6700, loss: 47.126232, accuracy: 0.949219\n",
      "step: 6800, loss: 64.519089, accuracy: 0.953125\n",
      "step: 6900, loss: 90.856285, accuracy: 0.898438\n",
      "step: 7000, loss: 48.045639, accuracy: 0.945312\n",
      "step: 7100, loss: 33.866531, accuracy: 0.941406\n",
      "step: 7200, loss: 61.432022, accuracy: 0.945312\n",
      "step: 7300, loss: 49.274719, accuracy: 0.960938\n",
      "step: 7400, loss: 417.244049, accuracy: 0.824219\n",
      "step: 7500, loss: 49.439537, accuracy: 0.953125\n",
      "step: 7600, loss: 60.833328, accuracy: 0.933594\n",
      "step: 7700, loss: 86.422104, accuracy: 0.906250\n",
      "step: 7800, loss: 41.415504, accuracy: 0.953125\n",
      "step: 7900, loss: 58.367096, accuracy: 0.933594\n",
      "step: 8000, loss: 34.321167, accuracy: 0.949219\n",
      "step: 8100, loss: 58.179939, accuracy: 0.929688\n",
      "step: 8200, loss: 32.810871, accuracy: 0.957031\n",
      "step: 8300, loss: 81.332016, accuracy: 0.910156\n",
      "step: 8400, loss: 80.751488, accuracy: 0.921875\n",
      "step: 8500, loss: 106.832993, accuracy: 0.914062\n",
      "step: 8600, loss: 40.951416, accuracy: 0.945312\n",
      "step: 8700, loss: 65.585785, accuracy: 0.910156\n",
      "step: 8800, loss: 75.933571, accuracy: 0.914062\n",
      "step: 8900, loss: 78.839577, accuracy: 0.914062\n",
      "step: 9000, loss: 58.441551, accuracy: 0.937500\n",
      "step: 9100, loss: 75.797920, accuracy: 0.914062\n",
      "step: 9200, loss: 72.750526, accuracy: 0.914062\n",
      "step: 9300, loss: 61.035515, accuracy: 0.941406\n",
      "step: 9400, loss: 42.616257, accuracy: 0.945312\n",
      "step: 9500, loss: 72.111092, accuracy: 0.941406\n",
      "step: 9600, loss: 56.236523, accuracy: 0.937500\n",
      "step: 9700, loss: 76.354080, accuracy: 0.937500\n",
      "step: 9800, loss: 98.822952, accuracy: 0.914062\n",
      "step: 9900, loss: 35.381634, accuracy: 0.960938\n",
      "step: 10000, loss: 38.291855, accuracy: 0.957031\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "\n",
    "    if step % display_step == 0:\n",
    "        pred = logistic_regression(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.904300\n"
     ]
    }
   ],
   "source": [
    "# Test model on validation set.\n",
    "pred = logistic_regression(x_test)\n",
    "\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.0111 %\n",
      "1: 0.1403 %\n",
      "2: 57.9664 %\n",
      "3: 37.1922 %\n",
      "4: 0.0002 %\n",
      "5: 0.0541 %\n",
      "6: 0.0521 %\n",
      "7: 0.0001 %\n",
      "8: 4.5832 %\n",
      "9: 0.0002 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_image = plt.imread('img/my/number1.jpg', format='jpeg')\n",
    "plt.imshow(my_image)\n",
    "\n",
    "# flatten images into one-dimensional vector\n",
    "num_pixels = my_image.shape[0] * my_image.shape[1] # find size of one-dimensional vector\n",
    "\n",
    "# Grayscale + Unit RGB\n",
    "gray = lambda rgb : np.dot(rgb[... , :3] , [0.299 , 0.587, 0.114]) \n",
    "\n",
    "test_image = np.array([gray(my_image)])\n",
    "\n",
    "plt.imshow(test_image[0])\n",
    "\n",
    "# flatten images into one-dimensional vector\n",
    "test_image = test_image.reshape(test_image.shape[0], num_pixels).astype('float32')\n",
    "test_image = test_image / 255\n",
    "\n",
    "# Predict\n",
    "result = logistic_regression(test_image)\n",
    "\n",
    "for i, v in enumerate(result[0]):\n",
    "    print(f'{i}: {v*100:.4f} %')\n",
    "\n",
    "# Result image\n",
    "image = np.reshape(my_image, (-1, 1))\n",
    "plt.imshow(test_image.reshape(28, 28), cmap='gray')\n",
    "plt.title(f'The number is: {tf.math.argmax(result, 1)}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
